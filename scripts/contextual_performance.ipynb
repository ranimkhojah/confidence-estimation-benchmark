{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d1dd375",
   "metadata": {},
   "source": [
    "# Initialization and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "73c00d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "import numpy as np\n",
    "from numpy import exp\n",
    "import collections\n",
    "from scipy import stats\n",
    "import math\n",
    "from statistics import stdev, mean\n",
    "from fractions import Fraction as fr\n",
    "import pandas as pd\n",
    "from statistics import median\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "INTENTS = [\"quirky\", \"factoid\", \"music\", \"query\", \"set\", \"remove\", \"sendemail\", \"negate\", \"repeat\", \"explain\"]\n",
    "NUM_INTENTS = len(INTENTS)\n",
    "NLU_NAMES = ['watson', 'luis', 'snips', 'rasa-sklearn', 'rasa-diet']\n",
    "COLORS = ['lightcoral', 'yellowgreen', 'deepskyblue', 'purple', 'mediumturquoise', 'mediumorchid', 'khaki', 'salmon', 'darkturquoise', 'gold']\n",
    "VERSION = 8\n",
    "NUM_SPLITS = 10\n",
    "\n",
    "def normalize(vector):\n",
    "    normalized_vector = [v/sum(vector) for v in vector]\n",
    "    return normalized_vector\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "def get_nlu_result_file(nlu, version, split_num):\n",
    "    nlu_result_file = '../results/' + nlu + '/v' + str(version) + '/' + nlu + '_split_' + str(split_num) + '_results_v' + str(version) + '.json'\n",
    "    return nlu_result_file\n",
    "\n",
    "def get_confidence_score(test, rank=0, nlu=\"watson\", level=\"rank\"):\n",
    "    \"\"\"\n",
    "    the output of this function differs depending on the level.\n",
    "    On rank level, the output is a single float that represents the confidence score of a rank \n",
    "    On model level, the output is a list of floats that includes the confidence scores of all ranks\n",
    "    \"\"\"\n",
    "    if level == \"rank\":\n",
    "        predicted_intent_conf = test['intent_ranking'][rank]['confidence'] # rasa\n",
    "        return predicted_intent_conf \n",
    "    \n",
    "    else: # level == \"model\"\n",
    "        predicted_confidences = [rank['confidence'] for rank in test['intent_ranking']]\n",
    "        return predicted_confidences\n",
    "    \n",
    "def get_accuracy(test, rank=0, nlu=\"watson\", level=\"rank\"):\n",
    "    \"\"\"\n",
    "    the output of this function differs depending on the level.\n",
    "    On rank level, the output is a single int that represents the instance-level accuracy of a rank\n",
    "    On model level, the output is a list of ints that includes the instance-level accuracies of all ranks\n",
    "    \"\"\"\n",
    "    correct_intent = test['correct_intent']\n",
    "    if level == \"rank\":\n",
    "        predicted_intent = test['intent_ranking'][rank]['name']\n",
    "        accuracy = int(correct_intent == predicted_intent)\n",
    "        return accuracy\n",
    "    else:\n",
    "        test_accuracies = []\n",
    "        test_accuracies = [rank['name'] == correct_intent for rank in test['intent_ranking']]\n",
    "        return test_accuracies\n",
    "    \n",
    "def normalize_test(nlu, test, normalized_scores):\n",
    "    \"\"\"\n",
    "    Updates scores in a test with normalized scores\n",
    "    Input: test (json object) with non-normalized scores + list of normalized scores \n",
    "    Output: new test with normalized scores\n",
    "    \"\"\"\n",
    "    for rank, score in enumerate(normalized_scores):\n",
    "        test['intent_ranking'][rank]['confidence'] = score\n",
    "    return test\n",
    "\n",
    "def normalize_data(nlu, data):\n",
    "    \"\"\" \n",
    "    This function normalizes all confidence scores in the json file\n",
    "    Output: list of json objects (treated as list of dictionaries)\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    for test in data:\n",
    "        try:\n",
    "            original_scores = get_confidence_score(test, nlu=nlu, level=\"model\")\n",
    "            normalized_scores = normalize(original_scores)\n",
    "        except:\n",
    "            normalized_scores = [0.1]*10\n",
    "        if test['text'] != '':\n",
    "            new_test = normalize_test(nlu, test, normalized_scores)\n",
    "            new_data.append(new_test)\n",
    "    return new_data\n",
    "\n",
    "def remove_empty_preds(data):\n",
    "    new_data = []\n",
    "    for test in data:\n",
    "        if test['text'] != '':\n",
    "            new_data.append(test)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4933b46",
   "metadata": {},
   "source": [
    "# Non-contextual performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d22efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83c2f160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nlu': 'watson',\n",
       "  'Mean': 0.9214412,\n",
       "  'Median': 0.9197166,\n",
       "  'standard deviation': 0.00233822},\n",
       " {'nlu': 'luis',\n",
       "  'Mean': 0.88889647,\n",
       "  'Median': 0.89300483,\n",
       "  'standard deviation': 0.00407424},\n",
       " {'nlu': 'snips',\n",
       "  'Mean': 0.89028629,\n",
       "  'Median': 0.89166467,\n",
       "  'standard deviation': 0.00217631},\n",
       " {'nlu': 'rasa-sklearn',\n",
       "  'Mean': 0.79020273,\n",
       "  'Median': 0.79561177,\n",
       "  'standard deviation': 0.00357872},\n",
       " {'nlu': 'rasa-diet',\n",
       "  'Mean': 0.81890462,\n",
       "  'Median': 0.81716341,\n",
       "  'standard deviation': 0.00330622}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nlus_f1_scores = []\n",
    "all_nlus_accuracies = []\n",
    "\n",
    "for i, nlu in enumerate(NLU_NAMES): \n",
    "    nlu_f1s = []\n",
    "    nlu_accs = []\n",
    "    for iteration in range(NUM_SPLITS):\n",
    "        split_num = iteration + 1\n",
    "        # load nlu results\n",
    "        nlu_result_file = get_nlu_result_file(nlu, VERSION, split_num)\n",
    "        data = remove_empty_preds(load_json(nlu_result_file))\n",
    "\n",
    "        predicted_intents = []\n",
    "        actual_intents = []\n",
    "        for test in data:\n",
    "            predicted = test['intent_ranking'][0]['name']\n",
    "            actual = test['correct_intent']\n",
    "\n",
    "            predicted_intents.append(predicted)\n",
    "            actual_intents.append(actual)\n",
    "\n",
    "\n",
    "        f1_score = metrics.f1_score(actual_intents, predicted_intents, average='macro')\n",
    "        accuracy_score = metrics.accuracy_score(actual_intents, predicted_intents)\n",
    "        nlu_f1s.append(f1_score)\n",
    "        nlu_accs.append(accuracy_score)\n",
    "    \n",
    "    \n",
    "    averaged_f1s = sum(nlu_f1s) / len(nlu_f1s)\n",
    "    sd_f1 = stdev(nlu_f1s)\n",
    "    median_f1 = np.median(nlu_f1s)\n",
    "    averaged_accs = sum(nlu_accs) / len(nlu_accs)\n",
    "    sd_acc = stdev(nlu_accs)\n",
    "    median_acc = np.median(nlu_accs)\n",
    "    all_nlus_f1_scores.append({'nlu': nlu, 'Mean': round(f1_score, 8), 'Median': round(median_f1, 8), 'standard deviation' : round(sd_f1, 8)})\n",
    "    all_nlus_accuracies.append({'nlu': nlu, 'Mean': round(accuracy_score, 8), 'Median': round(median_acc, 8), 'standard deviation' : round(sd_acc, 8)})\n",
    "        \n",
    "all_nlus_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a0a4e47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nlu': 'watson',\n",
       "  'Mean': 0.92287342,\n",
       "  'Median': 0.91996738,\n",
       "  'standard deviation': 0.00225174},\n",
       " {'nlu': 'luis',\n",
       "  'Mean': 0.88726089,\n",
       "  'Median': 0.890405,\n",
       "  'standard deviation': 0.0041371},\n",
       " {'nlu': 'snips',\n",
       "  'Mean': 0.88990639,\n",
       "  'Median': 0.89059627,\n",
       "  'standard deviation': 0.00233196},\n",
       " {'nlu': 'rasa-sklearn',\n",
       "  'Mean': 0.87263479,\n",
       "  'Median': 0.87866153,\n",
       "  'standard deviation': 0.00385567},\n",
       " {'nlu': 'rasa-diet',\n",
       "  'Mean': 0.90376399,\n",
       "  'Median': 0.89972545,\n",
       "  'standard deviation': 0.00385903}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nlus_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eda6e24",
   "metadata": {},
   "source": [
    "# Contextual performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f483f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "CONTEXT_STRENGTH = 0.8\n",
    "NUM_SAMPLES = 1000\n",
    "\n",
    "def random_context():\n",
    "    x = [random.uniform(1 - CONTEXT_STRENGTH, 1 + CONTEXT_STRENGTH) for _n in range(NUM_INTENTS)]\n",
    "    return normalize(x)\n",
    "\n",
    "def sample_from_context(context):\n",
    "    try:\n",
    "        drawn_sample = np.random.multinomial(1, context, size=1)[0]\n",
    "    except ValueError:\n",
    "        print('Exception when trying to draw sample from context', context)\n",
    "        raise\n",
    "    intent_index = np.where(drawn_sample == 1)[0][0]\n",
    "    return INTENTS[intent_index]\n",
    "\n",
    "def select_test(data, correct_intent):\n",
    "    tests = [\n",
    "        test\n",
    "        for test in data\n",
    "        if test['correct_intent'] == correct_intent\n",
    "    ]\n",
    "    return random.choice(tests)\n",
    "\n",
    "def get_confidence(intent, test):\n",
    "    for intent_ranking in test['intent_ranking']:\n",
    "        if intent_ranking['name'] == intent:\n",
    "            return intent_ranking['confidence']\n",
    "\n",
    "def predict_intent(data, context, correct_intent):\n",
    "    test = select_test(data, correct_intent)\n",
    "    linguistic_prediction = [get_confidence(intent, test) for intent in INTENTS]\n",
    "    contextualized_prediction = np.multiply(context, linguistic_prediction)\n",
    "    predicted_intent_index = np.argmax(contextualized_prediction)\n",
    "    predicted_intent = INTENTS[predicted_intent_index]\n",
    "    return predicted_intent\n",
    "    \n",
    "def measure_performance_for_nlu_and_split(nlu, split_num, contexts, correct_intents):\n",
    "    nlu_result_file = get_nlu_result_file(nlu, VERSION, split_num)\n",
    "    data = remove_empty_preds(load_json(nlu_result_file))\n",
    "    predicted_intents = [\n",
    "        predict_intent(data, context, correct_intent)\n",
    "        for context, correct_intent in zip(contexts, correct_intents)\n",
    "    ]\n",
    "    return {'accuracy': metrics.accuracy_score(correct_intents, predicted_intents),\n",
    "            'f1': metrics.f1_score(correct_intents, predicted_intents, average='macro')}\n",
    "\n",
    "contexts = [random_context() for _ in range(NUM_SAMPLES)]\n",
    "correct_intents = [sample_from_context(context) for context in contexts]\n",
    "\n",
    "performances_per_nlu = {\n",
    "    nlu: [\n",
    "        measure_performance_for_nlu_and_split(nlu, split_num, contexts, correct_intents)\n",
    "        for split_num in range(1, NUM_SPLITS + 1)\n",
    "    ]\n",
    "    for nlu in NLU_NAMES\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a682e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_per_nlu = {\n",
    "    nlu: mean([\n",
    "        split_performance['accuracy']\n",
    "        for split_performance in performances_per_nlu[nlu]\n",
    "    ])\n",
    "    for nlu in NLU_NAMES\n",
    "}\n",
    "\n",
    "accuracy_per_nlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92663663",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_per_nlu = {\n",
    "    nlu: mean([\n",
    "        split_performance['f1']\n",
    "        for split_performance in performances_per_nlu[nlu]\n",
    "    ])\n",
    "    for nlu in NLU_NAMES\n",
    "}\n",
    "\n",
    "f1_score_per_nlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c21d4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
