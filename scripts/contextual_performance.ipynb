{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79ab3db9",
   "metadata": {},
   "source": [
    "# Initialization and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af719c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "import numpy as np\n",
    "from numpy import exp\n",
    "import collections\n",
    "from scipy import stats\n",
    "import math\n",
    "from statistics import stdev, mean\n",
    "from fractions import Fraction as fr\n",
    "import pandas as pd\n",
    "from statistics import median\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "INTENTS = [\"quirky\", \"factoid\", \"music\", \"query\", \"set\", \"remove\", \"sendemail\", \"negate\", \"repeat\", \"explain\"]\n",
    "NUM_INTENTS = len(INTENTS)\n",
    "NLU_NAMES = ['watson', 'luis', 'snips', 'rasa-sklearn', 'rasa-diet']\n",
    "COLORS = ['lightcoral', 'yellowgreen', 'deepskyblue', 'purple', 'mediumturquoise', 'mediumorchid', 'khaki', 'salmon', 'darkturquoise', 'gold']\n",
    "VERSION = 8\n",
    "NUM_SPLITS = 10\n",
    "\n",
    "def normalize(vector):\n",
    "    normalized_vector = [v/sum(vector) for v in vector]\n",
    "    return normalized_vector\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path) as json_file:\n",
    "        data = json.load(json_file)\n",
    "    return data\n",
    "\n",
    "def get_nlu_result_file(nlu, version, split_num):\n",
    "    nlu_result_file = '../results/' + nlu + '/v' + str(version) + '/' + nlu + '_split_' + str(split_num) + '_results_v' + str(version) + '.json'\n",
    "    return nlu_result_file\n",
    "\n",
    "def get_confidence_score(test, rank=0, nlu=\"watson\", level=\"rank\"):\n",
    "    \"\"\"\n",
    "    the output of this function differs depending on the level.\n",
    "    On rank level, the output is a single float that represents the confidence score of a rank \n",
    "    On model level, the output is a list of floats that includes the confidence scores of all ranks\n",
    "    \"\"\"\n",
    "    if level == \"rank\":\n",
    "        predicted_intent_conf = test['intent_ranking'][rank]['confidence'] # rasa\n",
    "        return predicted_intent_conf \n",
    "    \n",
    "    else: # level == \"model\"\n",
    "        predicted_confidences = [rank['confidence'] for rank in test['intent_ranking']]\n",
    "        return predicted_confidences\n",
    "    \n",
    "def get_accuracy(test, rank=0, nlu=\"watson\", level=\"rank\"):\n",
    "    \"\"\"\n",
    "    the output of this function differs depending on the level.\n",
    "    On rank level, the output is a single int that represents the instance-level accuracy of a rank\n",
    "    On model level, the output is a list of ints that includes the instance-level accuracies of all ranks\n",
    "    \"\"\"\n",
    "    correct_intent = test['correct_intent']\n",
    "    if level == \"rank\":\n",
    "        predicted_intent = test['intent_ranking'][rank]['name']\n",
    "        accuracy = int(correct_intent == predicted_intent)\n",
    "        return accuracy\n",
    "    else:\n",
    "        test_accuracies = []\n",
    "        test_accuracies = [rank['name'] == correct_intent for rank in test['intent_ranking']]\n",
    "        return test_accuracies\n",
    "    \n",
    "def normalize_test(nlu, test, normalized_scores):\n",
    "    \"\"\"\n",
    "    Updates scores in a test with normalized scores\n",
    "    Input: test (json object) with non-normalized scores + list of normalized scores \n",
    "    Output: new test with normalized scores\n",
    "    \"\"\"\n",
    "    for rank, score in enumerate(normalized_scores):\n",
    "        test['intent_ranking'][rank]['confidence'] = score\n",
    "    return test\n",
    "\n",
    "def normalize_data(nlu, data):\n",
    "    \"\"\" \n",
    "    This function normalizes all confidence scores in the json file\n",
    "    Output: list of json objects (treated as list of dictionaries)\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    for test in data:\n",
    "        try:\n",
    "            original_scores = get_confidence_score(test, nlu=nlu, level=\"model\")\n",
    "            normalized_scores = normalize(original_scores)\n",
    "        except:\n",
    "            normalized_scores = [0.1]*10\n",
    "        if test['text'] != '':\n",
    "            new_test = normalize_test(nlu, test, normalized_scores)\n",
    "            new_data.append(new_test)\n",
    "    return new_data\n",
    "\n",
    "def remove_empty_preds(data):\n",
    "    new_data = []\n",
    "    for test in data:\n",
    "        if test['text'] != '':\n",
    "            new_data.append(test)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d6854",
   "metadata": {},
   "source": [
    "\n",
    "# Load NLU data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1d561ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nlu_data(nlu):\n",
    "    def load_nlu_data_for_split_num(split_num):\n",
    "        nlu_result_file = get_nlu_result_file(nlu, VERSION, split_num)\n",
    "        return remove_empty_preds(load_json(nlu_result_file))\n",
    "    return {\n",
    "        split_num: load_nlu_data_for_split_num(split_num)\n",
    "        for split_num in range(1, NUM_SPLITS+1)\n",
    "    }\n",
    "\n",
    "nlu_data = {\n",
    "    nlu: load_nlu_data(nlu)\n",
    "    for nlu in NLU_NAMES\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe62cf",
   "metadata": {},
   "source": [
    "# Non-contextual performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "77d87c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "949afd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nlu': 'watson',\n",
       "  'Mean': 0.9214412,\n",
       "  'Median': 0.9197166,\n",
       "  'standard deviation': 0.00233822},\n",
       " {'nlu': 'luis',\n",
       "  'Mean': 0.88889647,\n",
       "  'Median': 0.89300483,\n",
       "  'standard deviation': 0.00407424},\n",
       " {'nlu': 'snips',\n",
       "  'Mean': 0.89028629,\n",
       "  'Median': 0.89166467,\n",
       "  'standard deviation': 0.00217631},\n",
       " {'nlu': 'rasa-sklearn',\n",
       "  'Mean': 0.79020273,\n",
       "  'Median': 0.79561177,\n",
       "  'standard deviation': 0.00357872},\n",
       " {'nlu': 'rasa-diet',\n",
       "  'Mean': 0.81890462,\n",
       "  'Median': 0.81716341,\n",
       "  'standard deviation': 0.00330622}]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nlus_f1_scores = []\n",
    "all_nlus_accuracies = []\n",
    "\n",
    "for i, nlu in enumerate(NLU_NAMES): \n",
    "    nlu_f1s = []\n",
    "    nlu_accs = []\n",
    "    for iteration in range(NUM_SPLITS):\n",
    "        split_num = iteration + 1\n",
    "        data = nlu_data[nlu][split_num]\n",
    "\n",
    "        predicted_intents = []\n",
    "        actual_intents = []\n",
    "        for test in data:\n",
    "            predicted = test['intent_ranking'][0]['name']\n",
    "            actual = test['correct_intent']\n",
    "\n",
    "            predicted_intents.append(predicted)\n",
    "            actual_intents.append(actual)\n",
    "\n",
    "\n",
    "        f1_score = metrics.f1_score(actual_intents, predicted_intents, average='macro')\n",
    "        accuracy_score = metrics.accuracy_score(actual_intents, predicted_intents)\n",
    "        nlu_f1s.append(f1_score)\n",
    "        nlu_accs.append(accuracy_score)\n",
    "    \n",
    "    \n",
    "    averaged_f1s = sum(nlu_f1s) / len(nlu_f1s)\n",
    "    sd_f1 = stdev(nlu_f1s)\n",
    "    median_f1 = np.median(nlu_f1s)\n",
    "    averaged_accs = sum(nlu_accs) / len(nlu_accs)\n",
    "    sd_acc = stdev(nlu_accs)\n",
    "    median_acc = np.median(nlu_accs)\n",
    "    all_nlus_f1_scores.append({'nlu': nlu, 'Mean': round(f1_score, 8), 'Median': round(median_f1, 8), 'standard deviation' : round(sd_f1, 8)})\n",
    "    all_nlus_accuracies.append({'nlu': nlu, 'Mean': round(accuracy_score, 8), 'Median': round(median_acc, 8), 'standard deviation' : round(sd_acc, 8)})\n",
    "        \n",
    "all_nlus_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3735f244",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'nlu': 'watson',\n",
       "  'Mean': 0.92287342,\n",
       "  'Median': 0.91996738,\n",
       "  'standard deviation': 0.00225174},\n",
       " {'nlu': 'luis',\n",
       "  'Mean': 0.88726089,\n",
       "  'Median': 0.890405,\n",
       "  'standard deviation': 0.0041371},\n",
       " {'nlu': 'snips',\n",
       "  'Mean': 0.88990639,\n",
       "  'Median': 0.89059627,\n",
       "  'standard deviation': 0.00233196},\n",
       " {'nlu': 'rasa-sklearn',\n",
       "  'Mean': 0.87263479,\n",
       "  'Median': 0.87866153,\n",
       "  'standard deviation': 0.00385567},\n",
       " {'nlu': 'rasa-diet',\n",
       "  'Mean': 0.90376399,\n",
       "  'Median': 0.89972545,\n",
       "  'standard deviation': 0.00385903}]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nlus_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e3694",
   "metadata": {},
   "source": [
    "# Contextual performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "477f99c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "CONTEXT_STRENGTH = 0\n",
    "NUM_SAMPLES = 1000\n",
    "\n",
    "def random_context():\n",
    "    x = [random.uniform(1 - CONTEXT_STRENGTH, 1 + CONTEXT_STRENGTH) for _n in range(NUM_INTENTS)]\n",
    "    return normalize(x)\n",
    "\n",
    "def sample_from_context(context):\n",
    "    try:\n",
    "        drawn_sample = np.random.multinomial(1, context, size=1)[0]\n",
    "    except ValueError:\n",
    "        print('Exception when trying to draw sample from context', context)\n",
    "        raise\n",
    "    intent_index = np.where(drawn_sample == 1)[0][0]\n",
    "    return INTENTS[intent_index]\n",
    "\n",
    "def get_intent_ranking(data, utterance):\n",
    "    tests = [\n",
    "        test\n",
    "        for test in data\n",
    "        if test['text'] == utterance\n",
    "    ]\n",
    "    if len(tests) == 0:\n",
    "        raise Exception(f\"Found no tests for '{utterance}'\")\n",
    "    return tests[0]['intent_ranking']\n",
    "\n",
    "def get_confidence(intent, intent_ranking):\n",
    "    for hypothesis in intent_ranking:\n",
    "        if hypothesis['name'] == intent:\n",
    "            return hypothesis['confidence']\n",
    "\n",
    "def predict_intent(data, context, utterance):\n",
    "    intent_ranking = get_intent_ranking(data, utterance)\n",
    "    linguistic_prediction = [get_confidence(intent, intent_ranking) for intent in INTENTS]\n",
    "    contextualized_prediction = np.multiply(context, linguistic_prediction)\n",
    "    predicted_intent_index = np.argmax(contextualized_prediction)\n",
    "    predicted_intent = INTENTS[predicted_intent_index]\n",
    "    return predicted_intent\n",
    "    \n",
    "def measure_performance_for_nlu_and_split(nlu, split_num, contexts, utterances):\n",
    "    data = nlu_data[nlu][split_num]\n",
    "    try:\n",
    "        predicted_intents = [\n",
    "            predict_intent(data, context, utterance)\n",
    "            for context, correct_intent, utterance in zip(contexts, correct_intents, utterances)\n",
    "        ]\n",
    "    except Exception as exception:\n",
    "        raise Exception(f\"Failed to predict intents for nlu {nlu} split_num {split_num}: {exception}\")\n",
    "    return {'accuracy': metrics.accuracy_score(correct_intents, predicted_intents),\n",
    "            'f1': metrics.f1_score(correct_intents, predicted_intents, average='macro')}\n",
    "\n",
    "def pick_utterance(data, intent):\n",
    "    candidates = [test[\"text\"] for test in data if test[\"correct_intent\"] == intent]\n",
    "    return random.choice(candidates)\n",
    "    \n",
    "def pick_utterances_for_split(split_num, correct_intents):\n",
    "    data = nlu_data[NLU_NAMES[0]][split_num]\n",
    "    return [pick_utterance(data, intent) for intent in correct_intents]\n",
    "\n",
    "contexts = [random_context() for _ in range(NUM_SAMPLES)]\n",
    "correct_intents = [sample_from_context(context) for context in contexts]\n",
    "utterances_per_split = {\n",
    "    split_num: pick_utterances_for_split(split_num, correct_intents)\n",
    "    for split_num in range(1, NUM_SPLITS+1)\n",
    "}\n",
    "\n",
    "performances_per_nlu = {\n",
    "    nlu: [\n",
    "        measure_performance_for_nlu_and_split(nlu, split_num, contexts, utterances_per_split[split_num])\n",
    "        for split_num in range(1, NUM_SPLITS + 1)\n",
    "    ]\n",
    "    for nlu in NLU_NAMES\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b0304776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intent_with_only_context(context):\n",
    "    predicted_intent_index = np.argmax(context)\n",
    "    predicted_intent = INTENTS[predicted_intent_index]\n",
    "    return predicted_intent\n",
    "\n",
    "def measure_performance_for_context(contexts, correct_intents):\n",
    "    predicted_intents = [\n",
    "        predict_intent_with_only_context(context)\n",
    "        for context, correct_intent in zip(contexts, correct_intents)\n",
    "    ]\n",
    "    return {'accuracy': metrics.accuracy_score(correct_intents, predicted_intents),\n",
    "            'f1': metrics.f1_score(correct_intents, predicted_intents, average='macro')}\n",
    "\n",
    "performances_per_nlu['context'] = measure_performance_for_context(contexts, correct_intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "9a3d5cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'watson': 0.9151,\n",
       " 'luis': 0.9018,\n",
       " 'snips': 0.9016000000000001,\n",
       " 'rasa-sklearn': 0.8638,\n",
       " 'rasa-diet': 0.8961,\n",
       " 'context': 0.097}"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_per_nlu = {\n",
    "    nlu: mean([\n",
    "        split_performance['accuracy']\n",
    "        for split_performance in performances_per_nlu[nlu]\n",
    "    ])\n",
    "    for nlu in NLU_NAMES\n",
    "}\n",
    "accuracy_per_nlu['context'] = performances_per_nlu['context']['accuracy']\n",
    "accuracy_per_nlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "74df74b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'watson': 0.9165119645991944,\n",
       " 'luis': 0.9033679823513306,\n",
       " 'snips': 0.9024745574806611,\n",
       " 'rasa-sklearn': 0.8668917283054584,\n",
       " 'rasa-diet': 0.8978697123585072,\n",
       " 'context': 0.017684594348222428}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score_per_nlu = {\n",
    "    nlu: mean([\n",
    "        split_performance['f1']\n",
    "        for split_performance in performances_per_nlu[nlu]\n",
    "    ])\n",
    "    for nlu in NLU_NAMES\n",
    "}\n",
    "f1_score_per_nlu['context'] = performances_per_nlu['context']['f1']\n",
    "f1_score_per_nlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df106d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
